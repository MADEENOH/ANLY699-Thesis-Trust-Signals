{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 12074576,
          "sourceType": "datasetVersion",
          "datasetId": 7600650
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MADEENOH/ANLY699-Thesis-Trust-Signals/blob/main/Copy_of_LLM_Fine_Tuning_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1"
      ],
      "metadata": {
        "id": "5rv2ZEjBmjvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "6u_cabNwV4FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2"
      ],
      "metadata": {
        "id": "tz_t6pRtmpGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q unsloth\n",
        "\n"
      ],
      "metadata": {
        "id": "iOQ5a7D1DExG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3"
      ],
      "metadata": {
        "id": "JPT4wxjHmsEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "xAfdcKFtE06T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 4"
      ],
      "metadata": {
        "id": "hsPGxqvlmwAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# needed as this function doesn't like it when the lm_head has its size changed\n",
        "from unsloth import tokenizer_utils\n",
        "\n",
        "def do_nothing(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "tokenizer_utils.fix_untrained_tokens = do_nothing\n"
      ],
      "metadata": {
        "id": "KRtr01SIH0Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 5"
      ],
      "metadata": {
        "id": "w6jewL2Fmzwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"CUDA Capability: {torch.cuda.get_device_capability()}\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "# Number of classes (2 = binary classification: recommended or not)\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Model setup\n",
        "model_name = \"unsloth/mistral-7b-bnb-4bit\"\n",
        "load_in_4bit = True\n",
        "max_seq_length = 2048  # reduce if needed\n",
        "dtype = None  # Let Unsloth auto-detect\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        ")\n"
      ],
      "metadata": {
        "id": "8zyitpVvH5l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 6"
      ],
      "metadata": {
        "id": "t6HQC7acm4D8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 (Corrected)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "input_path = \"/content/data/Womens Clothing E-Commerce Reviews.csv\"\n",
        "data = pd.read_csv(input_path)\n",
        "\n",
        "# Keep only needed columns and remove missing rows\n",
        "data = data[['Review Text', 'Recommended IND']].dropna()\n",
        "data = data.rename(columns={'Review Text': 'text', 'Recommended IND': 'label'})\n",
        "\n",
        "# Use a sample as instructed\n",
        "data = data.sample(n=5000, random_state=42)\n",
        "\n",
        "# âœ… CHANGE: Map labels to text for more natural learning\n",
        "data['label'] = data['label'].map({0: \"Not Recommended\", 1: \"Recommended\"})\n",
        "\n",
        "# Check label distribution\n",
        "print(\"Label distribution:\")\n",
        "print(data['label'].value_counts())\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "# Train/validation split\n",
        "train_df, val_df = train_test_split(data, test_size=0.1, random_state=42)\n",
        "print(\"Train size:\", len(train_df), \"Val size:\", len(val_df))\n",
        "print(\"\\nExample validation data:\")\n",
        "print(val_df.head())"
      ],
      "metadata": {
        "id": "DR1jTmNlM0S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visuals EDA  \n",
        "\n",
        "This block contains the code for the Recommendation Distribution and the Review Length Histogram."
      ],
      "metadata": {
        "id": "cjsIRZUy1Bp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Set the style for our plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"viridis\")\n",
        "\n",
        "# Load the original data\n",
        "input_path = \"/content/data/Womens Clothing E-Commerce Reviews.csv\"\n",
        "original_data = pd.read_csv(input_path).dropna(subset=['Review Text', 'Recommended IND'])\n",
        "\n",
        "# --- Visualization 1: Distribution of Recommendation Outcomes ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.countplot(x='Recommended IND', data=original_data)\n",
        "plt.title('Distribution of Recommendation Outcomes', fontsize=16)\n",
        "plt.xlabel('Recommendation (0 = Not Recommended, 1 = Recommended)', fontsize=12)\n",
        "plt.ylabel('Number of Reviews', fontsize=12)\n",
        "ax.set_xticklabels(['Not Recommended', 'Recommended'])\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),\n",
        "                textcoords='offset points')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Visualization 2: Distribution of Review Text Lengths ---\n",
        "original_data['review_length'] = original_data['Review Text'].str.len()\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(original_data['review_length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Review Text Lengths', fontsize=16)\n",
        "plt.xlabel('Number of Characters in Review', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "djOwohxp1Y6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 7"
      ],
      "metadata": {
        "id": "opaUk_gCm8DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected Data Preparation Cell (Formatting + Tokenization)\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "# Define a clear, non-leaky prompt template for instruction-tuning\n",
        "prompt_template = \"\"\"Classify the sentiment of the following clothing review.\n",
        "Answer with only 'Recommended' or 'Not Recommended'.\n",
        "\n",
        "### Review:\n",
        "{}\n",
        "\n",
        "### Sentiment:\n",
        "{}\"\"\"\n",
        "\n",
        "# We need to add the EOS (End Of Sentence) token to the end of each example\n",
        "# so the model knows when to stop generating.\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# 1. Formatting Function\n",
        "def formatting_func(example):\n",
        "    # This creates the full prompt with the answer for training\n",
        "    full_text = prompt_template.format(example['text'], example['label']) + EOS_TOKEN\n",
        "    return { \"text\": full_text }\n",
        "\n",
        "# 2. Tokenizing Function\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the text, truncating sequences that are too long\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length, # This was defined in one of your first cells\n",
        "    )\n",
        "\n",
        "# Apply formatting to get a dataset with a 'text' column\n",
        "formatted_train_dataset = Dataset.from_pandas(train_df).map(formatting_func, remove_columns=list(train_df.columns))\n",
        "formatted_val_dataset = Dataset.from_pandas(val_df).map(formatting_func, remove_columns=list(val_df.columns))\n",
        "\n",
        "# Apply tokenization to that dataset.\n",
        "# `batched=True` makes this process much faster.\n",
        "# `remove_columns=[\"text\"]` deletes the now-unnecessary text column.\n",
        "train_dataset = formatted_train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "val_dataset = formatted_val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "print(\"âœ… Data preparation complete. The dataset now contains 'input_ids'.\")\n",
        "print(\"\\nExample of a tokenized sample:\")\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "ZtM3LAjhizjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 8 PEFT"
      ],
      "metadata": {
        "id": "53OxK3iAi-I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New Cell 2: PEFT Model Setup\n",
        "\n",
        "# We don't need to change the lm_head.\n",
        "# We target all linear layers for LoRA adaptation.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")"
      ],
      "metadata": {
        "id": "Gyn0L6SLi9pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diagnostic Cell\n",
        "import transformers\n",
        "import unsloth\n",
        "import torch\n",
        "\n",
        "print(\"--- Library Versions ---\")\n",
        "print(f\"Unsloth version:       {unsloth.__version__}\")\n",
        "print(f\"Transformers version:  {transformers.__version__}\")\n",
        "print(f\"Torch version:         {torch.__version__}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"--- TrainingArguments Documentation ---\")\n",
        "# This prints the official documentation for the TrainingArguments class\n",
        "# that is active in your notebook environment.\n",
        "from transformers import TrainingArguments\n",
        "help(TrainingArguments)"
      ],
      "metadata": {
        "id": "KxI5tM2KlUlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 19 Define TrainingArguments & Initialize Trainer"
      ],
      "metadata": {
        "id": "l-mspohXSmtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19 (Final Solution - Safe Mode)\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "print(\"Initializing Trainer in 'Safe Mode' to bypass the library bug.\")\n",
        "print(\"Evaluation arguments will be removed, and we will evaluate manually after training.\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    # We remove eval_dataset from the Trainer for now.\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"outputs\",\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=3, # You can set this back to 3\n",
        "        learning_rate=5e-5,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        bf16=True,\n",
        "        fp16=False,\n",
        "        seed=3407,\n",
        "        # All problematic evaluation and saving strategy arguments are removed.\n",
        "        # The model will save at the end of training.\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer initialized successfully in safe mode.\")"
      ],
      "metadata": {
        "id": "7fDsp6UKSqlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 20 Train the Model"
      ],
      "metadata": {
        "id": "fukhbdo7S_T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "qZIiK40xTFWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loss Curve\n",
        "\n",
        "This block creates the plot showing how your model's loss decreased during training."
      ],
      "metadata": {
        "id": "TfxUWHxF1wLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# The trainer object holds the history of the training process\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Extract loss and steps from the log history\n",
        "steps = []\n",
        "losses = []\n",
        "for log in log_history:\n",
        "    if 'loss' in log:\n",
        "        steps.append(log['step'])\n",
        "        losses.append(log['loss'])\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(steps, losses, marker='o', linestyle='-', label=\"Training Loss\")\n",
        "plt.title('Model Training Loss Curve', fontsize=16)\n",
        "plt.xlabel('Training Steps', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KWrQmWYJ11Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 21 Inference\n",
        "\n",
        "This part evaluates the model on the val set with batched inference"
      ],
      "metadata": {
        "id": "1uJYM31d7Cyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 21 (Corrected)\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Set up the prompt template for inference (the model fills in what's after \"Sentiment:\")\n",
        "inference_template = \"\"\"Classify the sentiment of the following clothing review.\n",
        "Answer with only 'Recommended' or 'Not Recommended'.\n",
        "\n",
        "### Review:\n",
        "{}\n",
        "\n",
        "### Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "# Get the original texts and true labels from the validation dataframe\n",
        "original_reviews = val_df['text'].tolist()\n",
        "true_labels = val_df['label'].tolist()\n",
        "predicted_labels = []\n",
        "\n",
        "# Ensure model is in eval mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Starting inference on the validation set...\")\n",
        "with torch.no_grad():\n",
        "    for review in tqdm(original_reviews):\n",
        "        # Format the prompt for inference\n",
        "        prompt = inference_template.format(review)\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        # Generate the text output\n",
        "        # max_new_tokens=5 is enough for \"Recommended\" or \"Not Recommended\"\n",
        "        outputs = model.generate(**inputs, max_new_tokens=5, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        # Decode the generated part\n",
        "        decoded_output = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "\n",
        "        # Clean the output and store it\n",
        "        cleaned_output = decoded_output.strip()\n",
        "        predicted_labels.append(cleaned_output)\n",
        "\n",
        "# --- Calculate Metrics ---\n",
        "print(\"\\nInference Complete. Calculating metrics...\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Generate full classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels, labels=[\"Recommended\", \"Not Recommended\"]))"
      ],
      "metadata": {
        "id": "Td0AQomwLX5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix (For a new cell after Cell 21)\n",
        "\n",
        "This block visualizes the final performance of your model from the classification report."
      ],
      "metadata": {
        "id": "mQqdWZtE2IFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# This code assumes the 'true_labels' and 'predicted_labels' lists from your\n",
        "# evaluation cell (Cell 21) are still in memory.\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels, labels=[\"Recommended\", \"Not Recommended\"])\n",
        "\n",
        "# Create a DataFrame for better labeling\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index=['True: Recommended', 'True: Not Recommended'],\n",
        "                     columns=['Pred: Recommended', 'Pred: Not Recommended'])\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix of Model Performance', fontsize=16)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8R5NZU962MTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "T9rUarvn-B8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final, merged model for easy inference\n",
        "# This combines the original Mistral model with your trained LoRA adapters\n",
        "model.save_pretrained_merged(\"final_sentiment_model\", tokenizer, save_method = \"merged_16bit\")"
      ],
      "metadata": {
        "id": "la4uu5WtLXUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "print(\"Loading the final, merged model from disk...\")\n",
        "# Load the final model that you saved locally\n",
        "model_path = \"final_sentiment_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# --- IMPORTANT ---\n",
        "# Define the name for your new model repository on the Hub.\n",
        "# You MUST replace 'your-hf-username' with your actual Hugging Face username.\n",
        "repo_id = \"MadeEnoh/mistral-7b-clothing-sentiment-v1\"\n",
        "\n",
        "print(f\"\\nUploading model to the Hugging Face Hub at: {repo_id}\")\n",
        "print(\"This may take several minutes as the model files are large...\")\n",
        "\n",
        "# Push both the model and the tokenizer to the Hub.\n",
        "# `private=True` makes the model visible only to you. You can remove this or set it to False to make it public later.\n",
        "model.push_to_hub(repo_id, private=True)\n",
        "tokenizer.push_to_hub(repo_id, private=True)\n",
        "\n",
        "print(f\"âœ… Success! Your model is now saved on the Hugging Face Hub.\")\n",
        "print(f\"You can view it at: https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "id": "uwb91d5LSG4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attempt at Keyphrases"
      ],
      "metadata": {
        "id": "60UUIhpHQ61f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "print(\"Starting key phrase analysis...\")\n",
        "\n",
        "# Load the dataset directly to ensure we have clean data\n",
        "try:\n",
        "    input_path = \"/content/data/Womens Clothing E-Commerce Reviews.csv\"\n",
        "    original_data = pd.read_csv(input_path).dropna(subset=['Review Text', 'Recommended IND'])\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Dataset file not found. Please ensure the path is correct.\")\n",
        "\n",
        "# Function to get the top n-grams (phrases) from a body of text\n",
        "def get_top_ngrams(corpus, n=None, ngram_range=(2, 2)):\n",
        "    # Use CountVectorizer to count the frequency of 2-word phrases\n",
        "    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Separate the review text into two categories\n",
        "recommended_corpus = original_data[original_data['Recommended IND'] == 1]['Review Text']\n",
        "not_recommended_corpus = original_data[original_data['Recommended IND'] == 0]['Review Text']\n",
        "\n",
        "# Get the top 15 most frequent phrases for each category\n",
        "top_phrases_recommended = get_top_ngrams(recommended_corpus, n=15)\n",
        "top_phrases_not_recommended = get_top_ngrams(not_recommended_corpus, n=15)\n",
        "\n",
        "# Convert the results to a DataFrame for easy plotting\n",
        "df_rec = pd.DataFrame(top_phrases_recommended, columns=['phrase', 'count'])\n",
        "df_not_rec = pd.DataFrame(top_phrases_not_recommended, columns=['phrase', 'count'])\n",
        "\n",
        "# --- Create the Visualization ---\n",
        "print(\"Generating plot...\")\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot for \"Recommended\" phrases\n",
        "sns.barplot(x='count', y='phrase', data=df_rec, ax=axes[0], palette='Greens_d')\n",
        "axes[0].set_title('Top 15 Key Phrases in \"Recommended\" Reviews', fontsize=16)\n",
        "axes[0].set_xlabel('Frequency', fontsize=12)\n",
        "axes[0].set_ylabel('Phrase (Trust Signal)', fontsize=12)\n",
        "\n",
        "# Plot for \"Not Recommended\" phrases\n",
        "sns.barplot(x='count', y='phrase', data=df_not_rec, ax=axes[1], palette='Reds_d')\n",
        "axes[1].set_title('Top 15 Key Phrases in \"Not Recommended\" Reviews', fontsize=16)\n",
        "axes[1].set_xlabel('Frequency', fontsize=12)\n",
        "axes[1].set_ylabel('') # Hide redundant label\n",
        "\n",
        "plt.suptitle('Discovered Trust Signals: An N-gram Analysis', fontsize=20, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nAnalysis complete.\")"
      ],
      "metadata": {
        "id": "AUPlEXD0Q5s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Story of the Charts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmpmyy0TRvrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ei5xQS8wSWk0"
      }
    }
  ]
}